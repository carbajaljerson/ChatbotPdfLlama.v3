from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
#from langchain import HuggingFacePipeline
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from langchain.llms import CTransformers
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import RetrievalQA

# Loads the appropriate tokenizer for the specified model checkpoint
'''tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                          use_auth_token=True,)'''

# Initializes a language model
'''model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                             device_map='auto',
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                             local_files_only=False)'''

        

def conversationChat(query, chain, history):
    '''
    Conducts a conversation by processing a user query using a conversation chain.

    Args:
        query (str): The user's input query.
        chain (callable): A function or callable object that takes a dictionary with a question
            and chat_history as input and returns a dictionary with an answer.
        history (list): A list of conversation history tuples, where each tuple contains the user's query
            and the system's response.

    Returns:
        str: The answer generated by the conversation chain for the user's query.

    The function processes the user's query by passing it to the provided conversation chain along with the
    conversation history. The resulting answer is then appended to the history list for reference.
    '''
    result = chain({"question": query, "chat_history": history})
    history.append((query, result["answer"]))
    return result["answer"]



def createConversationalChain(vectorStore,prompt):
    '''
    Creates a conversational chain for generating responses in a chat-based application.

    Args:
        vectorStore: A vector store from data structure used for retrieval-based chat models.

    Returns:
        ConversationalRetrievalChain: A conversational chain that combines a language model (LLM)
        for text generation with a retriever for context-based responses.

    This function initializes a conversational chain, which combines a language model (LLM) for text generation
    with a retriever for context-based responses. The chain is configured with specific parameters for text generation,
    retrieval, and memory management.
    '''
    '''pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )
    '''
    #llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.01})
    llm = CTransformers(model="./model/llama-2-7b-chat.ggmlv3.q4_0.bin",
                        streaming=True, 
                        callbacks=[StreamingStdOutCallbackHandler()],
                        model_type="llama", config={'temperature': 0.05} ,n_ctx=1024,max_tokens= 0)
   
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    #chain = ConversationalRetrievalChain.from_llm(llm=llm, 
    chain = RetrievalQA.from_chain_type(llm=llm, 
                                        chain_type='stuff',
                                        retriever=vectorStore.as_retriever(search_kwargs={"k": 2}),
                                        memory=memory, 
                                        chain_type_kwargs={"prompt": prompt})
    return chain